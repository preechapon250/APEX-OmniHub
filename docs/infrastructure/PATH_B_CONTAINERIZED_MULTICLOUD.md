# PATH B: CONTAINERIZED MULTI-CLOUD
**Alternative Implementation for OmniHub/TradeLine/APEX**

**Status:** âš ï¸ **ALTERNATIVE** (Maximum portability, higher complexity)

---

## OVERVIEW

**Strategy:** Migrate to containerized architecture with Kubernetes for true cloud-agnostic portability and maximum control.

**Why This Path?**
- âœ… **True Multi-Cloud:** Run on AWS, GCP, Azure, or on-premises without modification
- âœ… **Maximum Control:** Custom runtimes, fine-grained resource allocation
- âœ… **No Vendor Lock-In:** Can migrate between clouds with Terraform changes only
- âœ… **Enterprise-Ready:** Meets strict compliance requirements (on-prem, air-gapped)
- âœ… **Unlimited Scale:** No platform limits, scales to billions of users

**When to Choose This Path:**
- Need active-active multi-cloud (not just DR)
- Regulatory requirement for on-premises deployment
- User base > 5M users (serverless limits reached)
- Need custom runtimes or compute requirements beyond serverless
- Enterprise customers require self-hosted option

**Trade-Offs:**
- âŒ **Slower Time-to-Market:** 3-6 months implementation vs 6 weeks
- âŒ **Higher Costs:** $1000-2000/month minimum (Kubernetes cluster overhead)
- âŒ **Operational Overhead:** Need Kubernetes expertise, SRE team
- âŒ **Migration Risk:** Requires rewriting edge functions, testing migration

---

## ARCHITECTURE DIAGRAM

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ CLOUDFLARE / CLOUD CDN (Edge Layer)                         â”‚
â”‚ - Global anycast                                            â”‚
â”‚ - DDoS protection, WAF                                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â”‚
                     â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ KUBERNETES CLUSTER (GKE / EKS / AKS / On-Prem)              â”‚
â”‚                                                              â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚ INGRESS CONTROLLER (NGINX / Istio / Envoy)            â”‚ â”‚
â”‚  â”‚ - TLS termination, routing, rate limiting             â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚                      â”‚                                       â”‚
â”‚        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                       â”‚
â”‚        â”‚                            â”‚                       â”‚
â”‚        â–¼                            â–¼                       â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”             â”‚
â”‚  â”‚ FRONTEND     â”‚          â”‚ API GATEWAY     â”‚             â”‚
â”‚  â”‚ (NGINX)      â”‚          â”‚ (Node.js)       â”‚             â”‚
â”‚  â”‚ - Static SPA â”‚          â”‚ - Auth/authz    â”‚             â”‚
â”‚  â”‚ - CDN origin â”‚          â”‚ - Rate limiting â”‚             â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜          â”‚ - Audit logging â”‚             â”‚
â”‚                            â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜             â”‚
â”‚                                     â”‚                       â”‚
â”‚                       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”‚
â”‚                       â”‚                           â”‚         â”‚
â”‚                       â–¼                           â–¼         â”‚
â”‚              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚              â”‚ ORCHESTRATOR     â”‚      â”‚ READ API         â”‚ â”‚
â”‚              â”‚ (Node.js/Python) â”‚      â”‚ (Node.js)        â”‚ â”‚
â”‚              â”‚ - Intent parsing â”‚      â”‚ - Query service  â”‚ â”‚
â”‚              â”‚ - Workflow mgmt  â”‚      â”‚                  â”‚ â”‚
â”‚              â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚                       â”‚                                     â”‚
â”‚                       â–¼                                     â”‚
â”‚              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                          â”‚
â”‚              â”‚ MESSAGE QUEUE    â”‚                          â”‚
â”‚              â”‚ (RabbitMQ/NATS)  â”‚                          â”‚
â”‚              â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                          â”‚
â”‚                       â”‚                                     â”‚
â”‚                       â–¼                                     â”‚
â”‚              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”              â”‚
â”‚              â”‚ EXECUTOR POOL                â”‚              â”‚
â”‚              â”‚ (Kubernetes Jobs)            â”‚              â”‚
â”‚              â”‚ - Isolated containers        â”‚              â”‚
â”‚              â”‚ - Auto-scaling (HPA)         â”‚              â”‚
â”‚              â”‚ - Network policies (no egress)â”‚             â”‚
â”‚              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜              â”‚
â”‚                                                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                       â”‚
                       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ DATA LAYER (Managed or Self-Hosted)                         â”‚
â”‚                                                              â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚ PostgreSQL   â”‚  â”‚ Redis        â”‚  â”‚ Secrets Manager  â”‚  â”‚
â”‚  â”‚ (Cloud SQL/  â”‚  â”‚ (ElastiCache/â”‚  â”‚ (Vault)          â”‚  â”‚
â”‚  â”‚  RDS/Azure)  â”‚  â”‚  Memorystore)â”‚  â”‚                  â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                                                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                       â”‚
                       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ OBSERVABILITY LAYER                                          â”‚
â”‚  - Prometheus (metrics)                                      â”‚
â”‚  - Loki (logs)                                               â”‚
â”‚  - Tempo (traces)                                            â”‚
â”‚  - Grafana (dashboards)                                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## COMPONENT MAPPING

| Abstract Component | Containerized Implementation |
|--------------------|------------------------------|
| **Edge / CDN** | Cloudflare or Cloud CDN |
| **API Gateway** | Node.js app on K8s (Kong, Envoy, or custom) |
| **Orchestrator** | Node.js/Python app on K8s (Deployment) |
| **Executor Pool** | Kubernetes Jobs (ephemeral containers) |
| **Event Bus** | RabbitMQ, NATS, or Kafka on K8s |
| **Primary Database** | Cloud SQL / RDS / Azure Database (managed) OR CockroachDB (self-hosted multi-cloud) |
| **Cache** | Redis (ElastiCache / Memorystore / self-hosted) |
| **Secrets Manager** | HashiCorp Vault on K8s |
| **Blob Storage** | S3 / Cloud Storage / Azure Blob |
| **Observability** | Prometheus + Grafana + Loki + Tempo (all on K8s) |

---

## KUBERNETES ARCHITECTURE

### Namespaces

```yaml
# Namespace isolation for security + resource management
namespaces:
  - omnihub-prod         # Production workloads
  - omnihub-staging      # Staging workloads
  - omnihub-system       # Infrastructure (ingress, monitoring)
  - omnihub-data         # Databases, stateful workloads
```

### Network Policies (Zero Trust)

```yaml
# Default deny all traffic
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: default-deny-all
  namespace: omnihub-prod
spec:
  podSelector: {}
  policyTypes:
    - Ingress
    - Egress

---
# API Gateway â†’ Orchestrator (allowed)
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: api-gateway-to-orchestrator
  namespace: omnihub-prod
spec:
  podSelector:
    matchLabels:
      app: orchestrator
  policyTypes:
    - Ingress
  ingress:
    - from:
        - podSelector:
            matchLabels:
              app: api-gateway
      ports:
        - protocol: TCP
          port: 3000

---
# Executors â†’ Database (allowed via sidecar proxy)
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: executors-to-database
  namespace: omnihub-prod
spec:
  podSelector:
    matchLabels:
      app: executor
  policyTypes:
    - Egress
  egress:
    - to:
        - namespaceSelector:
            matchLabels:
              name: omnihub-data
      ports:
        - protocol: TCP
          port: 5432  # PostgreSQL

---
# Executors â†’ NO INTERNET (deny all other egress)
# Already covered by default-deny-all
```

### Pod Security Policies

```yaml
apiVersion: policy/v1beta1
kind: PodSecurityPolicy
metadata:
  name: restricted
spec:
  privileged: false
  allowPrivilegeEscalation: false
  requiredDropCapabilities:
    - ALL
  volumes:
    - 'configMap'
    - 'emptyDir'
    - 'projected'
    - 'secret'
  hostNetwork: false
  hostIPC: false
  hostPID: false
  runAsUser:
    rule: 'MustRunAsNonRoot'
  seLinux:
    rule: 'RunAsAny'
  fsGroup:
    rule: 'RunAsAny'
  readOnlyRootFilesystem: true
```

---

## SERVICE DEFINITIONS (KUBERNETES YAML)

### API Gateway Deployment

**File:** `k8s/api-gateway/deployment.yaml`
```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: api-gateway
  namespace: omnihub-prod
  labels:
    app: api-gateway
    version: v1.2.3
spec:
  replicas: 3
  strategy:
    type: RollingUpdate
    rollingUpdate:
      maxSurge: 1
      maxUnavailable: 0
  selector:
    matchLabels:
      app: api-gateway
  template:
    metadata:
      labels:
        app: api-gateway
        version: v1.2.3
      annotations:
        prometheus.io/scrape: "true"
        prometheus.io/port: "9090"
    spec:
      serviceAccountName: api-gateway
      securityContext:
        runAsNonRoot: true
        runAsUser: 1000
        fsGroup: 1000
      containers:
        - name: api-gateway
          image: gcr.io/omnihub/api-gateway:v1.2.3
          imagePullPolicy: IfNotPresent
          ports:
            - containerPort: 3000
              name: http
            - containerPort: 9090
              name: metrics
          env:
            - name: NODE_ENV
              value: "production"
            - name: POSTGRES_HOST
              valueFrom:
                secretKeyRef:
                  name: database-credentials
                  key: host
            - name: POSTGRES_PASSWORD
              valueFrom:
                secretKeyRef:
                  name: database-credentials
                  key: password
          resources:
            requests:
              memory: "256Mi"
              cpu: "250m"
            limits:
              memory: "512Mi"
              cpu: "500m"
          livenessProbe:
            httpGet:
              path: /health
              port: 3000
            initialDelaySeconds: 10
            periodSeconds: 10
          readinessProbe:
            httpGet:
              path: /health/deep
              port: 3000
            initialDelaySeconds: 15
            periodSeconds: 5
          securityContext:
            allowPrivilegeEscalation: false
            readOnlyRootFilesystem: true
            capabilities:
              drop:
                - ALL
---
apiVersion: v1
kind: Service
metadata:
  name: api-gateway
  namespace: omnihub-prod
spec:
  selector:
    app: api-gateway
  ports:
    - protocol: TCP
      port: 80
      targetPort: 3000
  type: ClusterIP
---
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: api-gateway-hpa
  namespace: omnihub-prod
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: api-gateway
  minReplicas: 3
  maxReplicas: 20
  metrics:
    - type: Resource
      resource:
        name: cpu
        target:
          type: Utilization
          averageUtilization: 70
    - type: Resource
      resource:
        name: memory
        target:
          type: Utilization
          averageUtilization: 80
```

### Executor Job Template

**File:** `k8s/executors/job-template.yaml`
```yaml
apiVersion: batch/v1
kind: Job
metadata:
  name: executor-{{WORKFLOW_ID}}
  namespace: omnihub-prod
  labels:
    app: executor
    workflow-id: {{WORKFLOW_ID}}
spec:
  ttlSecondsAfterFinished: 3600  # Auto-cleanup after 1 hour
  backoffLimit: 3
  template:
    metadata:
      labels:
        app: executor
        workflow-id: {{WORKFLOW_ID}}
    spec:
      restartPolicy: OnFailure
      serviceAccountName: executor-restricted
      securityContext:
        runAsNonRoot: true
        runAsUser: 1001
        fsGroup: 1001
      containers:
        - name: executor
          image: gcr.io/omnihub/executor:v1.2.3
          command: ["/app/executor"]
          args:
            - "--workflow-id={{WORKFLOW_ID}}"
            - "--action={{ACTION}}"
          env:
            - name: TRACE_ID
              value: "{{TRACE_ID}}"
            - name: IDEMPOTENCY_KEY
              value: "{{IDEMPOTENCY_KEY}}"
            - name: POSTGRES_HOST
              valueFrom:
                secretKeyRef:
                  name: database-credentials
                  key: host
          resources:
            requests:
              memory: "128Mi"
              cpu: "100m"
            limits:
              memory: "256Mi"
              cpu: "200m"
          securityContext:
            allowPrivilegeEscalation: false
            readOnlyRootFilesystem: true
            capabilities:
              drop:
                - ALL
```

---

## TERRAFORM STRUCTURE

### Directory Layout
```
terraform/
â”œâ”€â”€ modules/
â”‚   â”œâ”€â”€ kubernetes-cluster/
â”‚   â”‚   â”œâ”€â”€ aws-eks/
â”‚   â”‚   â”‚   â”œâ”€â”€ main.tf
â”‚   â”‚   â”‚   â”œâ”€â”€ variables.tf
â”‚   â”‚   â”‚   â””â”€â”€ outputs.tf
â”‚   â”‚   â”œâ”€â”€ gcp-gke/
â”‚   â”‚   â”‚   â”œâ”€â”€ main.tf
â”‚   â”‚   â”‚   â”œâ”€â”€ variables.tf
â”‚   â”‚   â”‚   â””â”€â”€ outputs.tf
â”‚   â”‚   â””â”€â”€ azure-aks/
â”‚   â”‚       â”œâ”€â”€ main.tf
â”‚   â”‚       â”œâ”€â”€ variables.tf
â”‚   â”‚       â””â”€â”€ outputs.tf
â”‚   â”œâ”€â”€ database/
â”‚   â”‚   â”œâ”€â”€ aws-rds/
â”‚   â”‚   â”œâ”€â”€ gcp-cloudsql/
â”‚   â”‚   â”œâ”€â”€ azure-database/
â”‚   â”‚   â””â”€â”€ cockroachdb/  # Multi-cloud option
â”‚   â”œâ”€â”€ redis/
â”‚   â”‚   â”œâ”€â”€ aws-elasticache/
â”‚   â”‚   â”œâ”€â”€ gcp-memorystore/
â”‚   â”‚   â””â”€â”€ self-hosted/
â”‚   â””â”€â”€ vault/
â”‚       â””â”€â”€ kubernetes/
â”œâ”€â”€ environments/
â”‚   â”œâ”€â”€ staging/
â”‚   â”‚   â”œâ”€â”€ aws/
â”‚   â”‚   â”‚   â”œâ”€â”€ main.tf
â”‚   â”‚   â”‚   â””â”€â”€ terraform.tfvars
â”‚   â”‚   â”œâ”€â”€ gcp/
â”‚   â”‚   â””â”€â”€ azure/
â”‚   â””â”€â”€ production/
â”‚       â”œâ”€â”€ aws/
â”‚       â”œâ”€â”€ gcp/
â”‚       â””â”€â”€ azure/
â””â”€â”€ helm-charts/
    â”œâ”€â”€ omnihub-api-gateway/
    â”œâ”€â”€ omnihub-orchestrator/
    â””â”€â”€ omnihub-infrastructure/
```

### Example: GKE Cluster Module

**File:** `terraform/modules/kubernetes-cluster/gcp-gke/main.tf`
```hcl
terraform {
  required_providers {
    google = {
      source  = "hashicorp/google"
      version = "~> 5.0"
    }
  }
}

resource "google_container_cluster" "omnihub" {
  name     = var.cluster_name
  location = var.region

  # We can't create a cluster with no node pool, so we create the smallest possible default
  # node pool and immediately delete it.
  remove_default_node_pool = true
  initial_node_count       = 1

  # Network configuration
  network    = var.vpc_network
  subnetwork = var.vpc_subnetwork

  # Master authorized networks (restrict API access)
  master_authorized_networks_config {
    cidr_blocks {
      cidr_block   = var.authorized_cidr
      display_name = "Operator network"
    }
  }

  # Private cluster (nodes have no public IPs)
  private_cluster_config {
    enable_private_nodes    = true
    enable_private_endpoint = false
    master_ipv4_cidr_block  = "172.16.0.0/28"
  }

  # Workload identity (secure service account access)
  workload_identity_config {
    workload_pool = "${var.project_id}.svc.id.goog"
  }

  # Binary authorization (enforce signed images only)
  binary_authorization {
    evaluation_mode = "PROJECT_SINGLETON_POLICY_ENFORCE"
  }

  # Security hardening
  security_posture_config {
    mode               = "ENTERPRISE"
    vulnerability_mode = "VULNERABILITY_ENTERPRISE"
  }

  # Logging & monitoring
  logging_config {
    enable_components = ["SYSTEM_COMPONENTS", "WORKLOADS"]
  }

  monitoring_config {
    enable_components = ["SYSTEM_COMPONENTS", "WORKLOADS"]
    managed_prometheus {
      enabled = true
    }
  }

  # Maintenance window (Sunday 2-6 AM UTC)
  maintenance_policy {
    daily_maintenance_window {
      start_time = "02:00"
    }
  }
}

# Node pool for system workloads (monitoring, ingress, etc.)
resource "google_container_node_pool" "system" {
  name       = "system-pool"
  location   = var.region
  cluster    = google_container_cluster.omnihub.name
  node_count = 2

  autoscaling {
    min_node_count = 2
    max_node_count = 5
  }

  node_config {
    preemptible  = false
    machine_type = "e2-standard-2"

    labels = {
      pool = "system"
    }

    taint {
      key    = "workload-type"
      value  = "system"
      effect = "NO_SCHEDULE"
    }

    oauth_scopes = [
      "https://www.googleapis.com/auth/cloud-platform"
    ]

    workload_metadata_config {
      mode = "GKE_METADATA"
    }
  }
}

# Node pool for application workloads
resource "google_container_node_pool" "application" {
  name       = "application-pool"
  location   = var.region
  cluster    = google_container_cluster.omnihub.name
  node_count = 3

  autoscaling {
    min_node_count = 3
    max_node_count = 20
  }

  node_config {
    preemptible  = false
    machine_type = "e2-standard-4"

    labels = {
      pool = "application"
    }

    oauth_scopes = [
      "https://www.googleapis.com/auth/cloud-platform"
    ]

    workload_metadata_config {
      mode = "GKE_METADATA"
    }
  }
}

# Node pool for executor workloads (batch jobs, isolated)
resource "google_container_node_pool" "executors" {
  name       = "executor-pool"
  location   = var.region
  cluster    = google_container_cluster.omnihub.name
  node_count = 1

  autoscaling {
    min_node_count = 1
    max_node_count = 50  # Can scale high for burst workloads
  }

  node_config {
    preemptible  = true  # Cost savings (executors are stateless, can tolerate preemption)
    machine_type = "e2-standard-2"

    labels = {
      pool = "executor"
    }

    taint {
      key    = "workload-type"
      value  = "executor"
      effect = "NO_SCHEDULE"
    }

    oauth_scopes = [
      "https://www.googleapis.com/auth/cloud-platform"
    ]

    workload_metadata_config {
      mode = "GKE_METADATA"
    }
  }
}

output "cluster_name" {
  value = google_container_cluster.omnihub.name
}

output "cluster_endpoint" {
  value     = google_container_cluster.omnihub.endpoint
  sensitive = true
}

output "cluster_ca_certificate" {
  value     = google_container_cluster.omnihub.master_auth[0].cluster_ca_certificate
  sensitive = true
}
```

---

## DISASTER RECOVERY (MULTI-REGION)

### Active-Passive Multi-Region

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ US-EAST-1 (PRIMARY)                                        â”‚
â”‚                                                            â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”          â”‚
â”‚  â”‚ GKE Cluster      â”‚      â”‚ Cloud SQL        â”‚          â”‚
â”‚  â”‚ (3 nodes min)    â”‚â”€â”€â”€â”€â”€â–ºâ”‚ (Read-Write)     â”‚          â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜          â”‚
â”‚                                     â”‚ Async Replication   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                      â”‚
                                      â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ EU-WEST-1 (STANDBY)                                        â”‚
â”‚                                                            â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”          â”‚
â”‚  â”‚ GKE Cluster      â”‚      â”‚ Cloud SQL        â”‚          â”‚
â”‚  â”‚ (1 node, scaled  â”‚â”€â”€â”€â”€â”€â–ºâ”‚ (Read-Only)      â”‚          â”‚
â”‚  â”‚  down)           â”‚      â”‚                  â”‚          â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜          â”‚
â”‚                                                            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â–²
                     â”‚
         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
         â”‚ Global Load Balancer    â”‚
         â”‚ (Cloud CDN + health     â”‚
         â”‚  checks)                â”‚
         â”‚ - Routes to PRIMARY     â”‚
         â”‚ - Fails over to STANDBY â”‚
         â”‚   if PRIMARY down       â”‚
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Failover Process:**
1. Health check detects primary region failure (2 consecutive failures)
2. Global LB routes traffic to standby region
3. Promote standby database to read-write
4. Scale up standby K8s cluster (1 node â†’ 3 nodes)
5. Alert on-call engineer

**Automation:**
```bash
# scripts/failover-to-standby.sh
#!/bin/bash
set -e

REGION=${1:-eu-west-1}

echo "Initiating failover to $REGION..."

# 1. Promote standby database to primary
gcloud sql instances promote-replica omnihub-db-replica-$REGION

# 2. Scale up standby Kubernetes cluster
kubectl --context=gke-$REGION scale deployment --all --replicas=3 -n omnihub-prod

# 3. Update DNS to point to standby region (if using Route53)
# aws route53 change-resource-record-sets --hosted-zone-id Z123 --change-batch file://dns-failover.json

# 4. Send alert
curl -X POST $SLACK_WEBHOOK_URL \
  -d '{"text":"ðŸš¨ FAILOVER: Primary region failed, now serving from '"$REGION"'"}'

echo "Failover complete. RTO: $(date)"
```

---

## COST ESTIMATE

### Staging Environment (GKE on GCP)

| Component | Configuration | Cost/Month |
|-----------|---------------|------------|
| **GKE Cluster** | 3 nodes (e2-standard-2) | $100 |
| **Cloud SQL** | db-f1-micro (1 vCPU, 3.75GB) | $25 |
| **Redis (Memorystore)** | 1GB | $30 |
| **Load Balancer** | 1 forwarding rule | $20 |
| **Cloud Storage** | 100GB | $5 |
| **Monitoring** | Stackdriver | $10 |
| **Total** | | **~$190/month** |

### Production Environment (GKE on GCP)

| Component | Configuration | Cost/Month |
|-----------|---------------|------------|
| **GKE Cluster** | 10 nodes (e2-standard-4) | $650 |
| **Cloud SQL** | db-n1-standard-2 (HA) | $250 |
| **Redis** | 5GB standard | $150 |
| **Load Balancer** | Global LB + CDN | $80 |
| **Cloud Storage** | 1TB | $20 |
| **Monitoring** | Stackdriver | $50 |
| **Total** | | **~$1200/month** |

**Multi-Region (Active-Passive):** ~$1800/month (add standby region at 50% capacity)

---

## ROLLOUT TIMELINE

### Month 1: Foundation
- Week 1-2: Terraform modules for GKE/EKS/AKS
- Week 3-4: Deploy staging cluster, migrate edge functions to Docker

### Month 2: Migration
- Week 1: Migrate database from Supabase to Cloud SQL/RDS
- Week 2: Rewrite edge functions as containerized microservices
- Week 3: Integration testing
- Week 4: Load testing, performance tuning

### Month 3: Production Rollout
- Week 1-2: Deploy production cluster
- Week 3: Blue-green deployment (dual-run serverless + containers)
- Week 4: Traffic shift to containers (10% â†’ 50% â†’ 100%)

### Month 4-6: Optimization & Multi-Cloud
- Implement active-passive multi-region
- Add second cloud provider (AWS or Azure)
- Chaos engineering drills
- Performance optimization

**Total Time:** 4-6 months

---

## PROS & CONS

### Pros âœ…
- **True Multi-Cloud:** Can run on any Kubernetes platform
- **Maximum Control:** Fine-grained resource management, custom runtimes
- **No Vendor Lock-In:** Migrate between clouds with Terraform changes
- **Unlimited Scale:** No platform limits
- **Enterprise-Ready:** Meets air-gap, on-prem requirements
- **Advanced Features:** Service mesh, policy as code, canary deployments

### Cons âŒ
- **Slow Time-to-Market:** 4-6 months vs 6 weeks for serverless
- **High Costs:** $1200+/month vs $300-500 for serverless
- **Operational Overhead:** Requires Kubernetes expertise, SRE team
- **Migration Risk:** Full rewrite of edge functions, data migration
- **Complexity:** More moving parts, harder to debug

---

## WHEN TO CHOOSE PATH B

Choose containerized multi-cloud IF:
- [ ] User base > 5M users (serverless limits reached)
- [ ] Need active-active multi-cloud (not just DR)
- [ ] Regulatory requirement for on-premises deployment
- [ ] Enterprise customers require self-hosted option
- [ ] Need custom runtimes beyond Node.js/Deno
- [ ] Have dedicated SRE team for Kubernetes operations

Otherwise, **start with Path A** (serverless) and migrate later if needed.

---

**Document Status:** âœ… COMPLETE
**Recommendation:** Start with PATH A, migrate to PATH B only if necessary
